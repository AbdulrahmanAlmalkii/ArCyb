{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxJppOtimQuslHzmfaSMZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulrahmanAlmalkii/ArCyb/blob/main/ArCyb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXyZI6aWFbfQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/aub-mind/arabert/\n",
        "!pip install tnkeeh\n",
        "!pip install gensim\n",
        "!pip install  farasapy\n",
        "!pip install pyarabic\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from snowballstemmer import stemmer\n",
        "from nltk.featstruct import remove_variables\n",
        "from nltk.tokenize.casual import remove_handles\n",
        "import tnkeeh as tn \n",
        "from datasets import load_dataset\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "stop_words = pd.read_csv(r'./stopwords.csv')\n",
        "\n",
        "def preprocess(text):\n",
        "    '''\n",
        "    text is an arabic string input\n",
        "\n",
        "    the preprocessed text is returned\n",
        "    '''\n",
        "\n",
        "    # remove punctuations\n",
        "    # remove Tashkeel\n",
        "    # remove longation\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ا\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    text = re.sub(\"n\", \" \", text)\n",
        "    text = re.sub(\"انو\", \"انه\", text)\n",
        "    text = re.sub(\"دا\", \"هذا\", text)\n",
        "    text = re.sub(\"ذا\", \"هذا\", text)\n",
        "    text = re.sub(\"ليه\", \"لماذا\", text)\n",
        "    text = re.sub(\"ليش\", \"لماذا\", text)\n",
        "    text = re.sub(\"صلي\", \"صلا\", text)\n",
        "    text = re.sub(\"صلى\", \"صلا\", text)\n",
        "    text = re.sub(r'\\((.*?)\\)', '', text)\n",
        "    text = re.sub('[A-Za-z]+', '', text)\n",
        "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+', '', text)\n",
        "\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words and not word.isdigit())\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df = pd.read_excel(r\"4140 final dataset.xlsx\")\n",
        "df = df.dropna()\n",
        "df = df.sample(frac=1)\n",
        "print(df.head())\n",
        "print(df.sample(5))\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "cleander = tn.Tnkeeh(remove_diacritics=True, remove_english=True, remove_twitter_meta=True, remove_tatweel=True,\n",
        "                     remove_html_elements=True, remove_special_chars=True, remove_long_words=True,\n",
        "                     remove_repeated_chars=True\n",
        "                     , remove_links=True)\n",
        "cleaned_dataset = cleander.clean_data_frame(df, 'Tweets')\n",
        "\n",
        "\n",
        "def cleaning(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "\n",
        "    text = re.sub(\"معوق\", \"معاق\", text)\n",
        "    text = re.sub(\"اعاقة\", \"معاق\", text)\n",
        "    text = re.sub(\"تبكي\", \"بكى\", text)\n",
        "    text = re.sub(\"يبكي\", \"بكى\", text)\n",
        "    text = re.sub(\"ببكي\", \"بكى\", text)\n",
        "    text = re.sub(\"بكا\", \"بكى\", text)\n",
        "    text = re.sub(\"شذوذ\", \"شذ\", text)\n",
        "    text = re.sub(\"شاذ\", \"شذ\", text)\n",
        "    text = re.sub(\"تصيح\", \"صاح\", text)\n",
        "    text = re.sub(\"بصيح\", \"صاح\", text)\n",
        "    text = re.sub(\"يصيح\", \"صاح\", text)\n",
        "    text = re.sub(\"صياح\", \"صاح\", text)\n",
        "    text = re.sub(\"يتدلع\", \"دلع\", text)\n",
        "    text = re.sub(\"تتدلع\", \"دلع\", text)\n",
        "    text = re.sub(\"احنا\", \"حنى\", text)\n",
        "    text = re.sub(\"منا\", \"من\", text)\n",
        "    text = re.sub(\"كانو\", \"كانه\", text)\n",
        "    text = re.sub(\"منو\", \"منه\", text)\n",
        "    text = re.sub(\"زيو\", \"زيه\", text)\n",
        "    text = re.sub(\"انو\", \"انه\", text)\n",
        "    text = re.sub(\"هذا العبد\", \"عبد\", text)\n",
        "    text = re.sub(\"يا العبد\", \"عبد\", text)\n",
        "    text = re.sub(\"العبد\", \"عبد\", text)\n",
        "\n",
        "    text = re.sub('[A-Za-z]+', ' ', text)\n",
        "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+', ' ', text)\n",
        "    text = re.sub('[0-9]+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_dataset['Tweets'] = cleaned_dataset['Tweets'].apply(cleaning)\n",
        "\n",
        "feature = df.Tweets.astype(str)\n",
        "\n",
        "target = df.label.astype(str)\n",
        "\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "feature = np.array(cleaned_dataset['Tweets'])\n",
        "for i in range(len(feature)):\n",
        "    feature[i] = arabert_prep.preprocess(feature[i])\n",
        "    feature[i] = preprocess(feature[i])\n",
        "    feature[i] = remove_emoji(feature[i])\n",
        "\n",
        "\n",
        "def remove_stop_words(lines):\n",
        "    stop_words = ['+']\n",
        "    results = []\n",
        "    for text in lines:\n",
        "        tmp = text.split(' ')\n",
        "        for x in range(0, len(tmp)):\n",
        "            for st_w in stop_words:\n",
        "                if st_w in tmp[x]:\n",
        "                    tmp[x] = ''\n",
        "        results.append(\" \".join(tmp))\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "vWoHpRUiI-RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size=.2, stratify=target )\n",
        "\n",
        "\n",
        "pipe = make_pipeline(CountVectorizer(),\n",
        "                    TfidfTransformer(),\n",
        "                     MLPClassifier(solver='adam', hidden_layer_sizes=(30,66,66, 30),\n",
        "                                   activation='logistic', learning_rate='adaptive', shuffle=True,  validation_fraction=0.01, max_iter=600)\n",
        "                     )\n",
        "\n",
        "mlp_model = pipe.fit(X_train,Y_train)\n",
        "\n",
        "prediction = mlp_model.predict(X_test)\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
        "print(classification_report(Y_test, prediction))\n",
        "print(confusion_matrix(prediction, Y_test))"
      ],
      "metadata": {
        "id": "ZiyyHgJUbZ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "tf_vec = TfidfVectorizer()\n",
        "X = tf_vec.fit_transform(feature)\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size=.2, random_state=16,  stratify=target )\n",
        "\n",
        "svc = SVC()\n",
        "svc.fit(X_train, Y_train)\n",
        "preds = svc.predict(X_test)\n",
        "acc = np.mean(preds == Y_test)\n",
        "print('SVC model accuracy: {}'.format(acc*100))\n",
        "\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, preds):.2f}\")\n",
        "print(classification_report(Y_test, preds))\n",
        "\n",
        "print(confusion_matrix(preds, Y_test))\n"
      ],
      "metadata": {
        "id": "MEEdmJe-i5WN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}